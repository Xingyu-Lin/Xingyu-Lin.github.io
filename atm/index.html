<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ATM</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Any-point Trajectory Modeling for Policy Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://alvinwen428.github.io/" target="_blank">Chuan Wen</a><sup>1,2</sup>*,
            </span>
            <span class="author-block">
              <a href="https://xingyu-lin.github.io" target="_blank">Xingyu Lin</a><sup>1</sup>*,
            </span>
            <span class="author-block">
              <a href="https://www.johnrso.xyz/" target="_blank">John So</a><sup>3</sup>*,
            </span><br>
            <span class="author-block">
              <a href="https://ck-kai.github.io/" target="_blank">Kai Chen</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cse.cuhk.edu.hk/~qdou/" target="_blank">Qi Dou</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="https://yang-gao.weebly.com/" target="_blank">Yang Gao</a><sup>2,4,5</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UC Berkeley,</span>
            <span class="author-block"><sup>2</sup>IIIS, Tsinghua University,</span>
            <span class="author-block"><sup>3</sup>Stanford University,</span><br>
            <span class="author-block"><sup>4</sup>Shanghai Artificial Intelligence Laboratory,</span>
            <span class="author-block"><sup>5</sup>Shanghai Qi Zhi Institute,</span>
            <span class="author-block"><sup>6</sup>CUHK</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2401.00025"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="./files/appendix.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Appendix</span>
                  </a>
              </span> -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <img src="./static/images/pull.png" alt="teaser" class="teaser-image">
        <h2 class="subtitle has-text-centered">
          ATM learns manipulation policies from few demonstrations using 2D point track proposals learned from large actionless video datasets.
        </h2>
      </div>
    </div>
  </div>
  <br>
</section>

<section class="hero is-light">
  <br>
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Learning from demonstration is a powerful method for teaching robots new skills, and more demonstration data often improves policy learning. However, the high cost of collecting demonstration data is a significant bottleneck. Videos, as a rich data source, contain knowledge of behaviors, physics, and semantics, but extracting control-specific information from them is challenging due to the lack of action labels. In this work, we introduce a novel framework, Any-point Trajectory Modeling (ATM), that utilizes video demonstrations by pre-training a trajectory model to predict future trajectories of arbitrary points within a video frame. Once trained, these trajectories provide detailed control guidance, enabling the learning of robust visuomotor policies with minimal action-labeled data. Across the 130 language-conditioned tasks we evaluated in simulation, ATM outperforms strong baselines by 80% in average.
          </p><br>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-multiline">
      <div class="column is-12">


        <h2 class="title is-4">Visualization of Policy Rollout</h2>
      </div>
    </div>
    <!-- <div class="columns is-multiline">
      <div class="column is-12 has-text-left">
          I keep the below <b>images</b> and instead put it in a carousel for different tasks (similar to above) to highlight certain behaviors.
      </div>
      <div class="column is-12 has-text-left">
        <img src="./static/images/track_vis.png" alt="teaser" class="teaser-image">
      </div>
    </div> -->
    <div class="columns is-multiline">
      <div class="column is-12 has-text-left">
        <p>
          Below, we show the track-guided manipulation policy. After we pre-train a track prediction model on video data, we can train policies by learning from limited human demonstration (e.g. 10 demonstrations per task). During policy learning, we sample a fixed set of 64 points across the thrid-person camera view and the wrist view. You can see the predicted tracks indicated by the colored curves. The curves indicate the locations (in the camera frame) that the points should go to in future time steps.
          With dense supervision from the predicted tracks, our trained policies are able to perform long-horizon tasks, reasoning on objects, spatial locations and language instructions.
        </p>
        <br>
        <p>
          Use the tabs and the dropdown menu to select the task suite in the benchmark the language instruction.
        </p>
      </div>
    </div>
    <div class="tabs is-toggle is-fullwidth" id="policyTaskTabs">
      <ul>
        <li class="tablinks" id="clickDefault" onclick="switchTab(event, 'long')"><a><b>LIBERO-Long</b></a></li>
        <li class="tablinks" onclick="switchTab(event, 'spatial')"><a><b>LIBERO-Spatial</b></a></li>
        <li class="tablinks" onclick="switchTab(event, 'object')"><a><b>LIBERO-Object</b></a></li>
        <li class="tablinks" onclick="switchTab(event, 'goal')"><a><b>LIBERO-Goal</b></a></li>
      </ul>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <div class="field">
          <div class="control">
            <div class="select">
              <select id="dropdown2" onchange="changeImage()">
              </select>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
      <div class="column is-10">
        <video id="displayed-image" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/libero-spatial/env_0.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
    <br>


    <div class="columns is-centered has-text-left">
      <div class="column">
        <h2 class="title is-4">Quantitative Results</h2>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
      <div class="column is-12">
        <p>
<!--          We compare ATM to video pre-training policy learning baselines using LIBERO, a simulation benchmark for language-conditioned manipulation featuring over 100 tasks across 5 suites, each emphasizing a different generalization axis. We emphasize learning from few action labelled demonstrations: when applicable, we first pre-train on the entire action-less video dataset for each suite. Then, we perform behavior cloning on 10 demonstrations per task. We report the success metric across tasks in the suite.-->
          We evaluate our method on a challenging simulation benchmark comprising 130 language-conditioned manipulation tasks. Our experiments demonstrate that trajectory-guided policies significantly surpass various strong baselines in video pre-training, achieving an average success rate of 63% compared to the highest success rate of 37% by previous methods, marking an improvement of over 80%. ATM's performance is also comparable to BC with 5x more training demonstrations (the BC-Full-Trainset indicated by the black dashed line).
        </p>
        <br>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
      <div class="column is-10">
        <img src="./static/images/main_results.png" alt="teaser" class="teaser-image"><br>
      </div>
    </div>
    <div class="columns is-centered is-multiline">
      <div class="column is-12">
        <h2 class="title is-5">Which is better for video pre-training? Generative Video Model vs. Trajectory Model </h2>
      </div>
      <div class="column is-12 has-text-left">
        <p>
          To better understand the advantages of ATM's track subgoals, we compare them qualitatively to image subgoals generated by <b>UniPi (left)</b>. To decouple the advantages of open-loop and closed-loop video generation, we additionally instantiate <b>UniPi-Replan (right)</b>, which proposes new image subgoals every 8 actions.
        </p>
        <br>
        <p>
          Qualitatively, UniPi suffers from motor control failure caused by a lack of fine-grained details in image subgoals. UniPi-Replan additionally experiences failures in image generation, generating noisy images when out of distribution, or generating images that correspond to a different task.
        </p>
      </div>
    </div>
    <div class="container affordance-viz">
      <div class="carousel">
        <div class="item">
          <div class="item-container">
            <p>
              <strong>Pick up the alphabet soup and place it in the basket</strong>: UniPi-Replan fails to pick up the soup. It is difficult to determine whether the generated image subgoals reach for the soup can in the back, or the carton in the front.
            </p>
          </div>
          <div class="item-container">
            <video class="validation" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/unipi/obj-0-succ.mp4"
                      type="video/mp4">
            </video>
            <video class="validation" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/unipi-replan/obj-0-generation.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item-container">
            <div class="item-label">
              <p><strong>UniPi</strong></p>
            </div>
            <div class="item-label">
              <p><strong>UniPi-Replan</strong></p>
            </div>
          </div>
        </div>
        <div class="item">
          <div class="item-container">
            <p>
              <strong>Pick up the bbq sauce and place it in the basket</strong>: UniPi fails to pick up the bbq sauce, as image subgoals lack finer details relevant to motor control.
            </p>
          </div>
          <div class="item-container">
            <video class="validation" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/unipi/obj-1-motor.mp4"
                      type="video/mp4">
            </video>
            <video class="validation" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/unipi-replan/obj-1-succ.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item-container">
            <div class="item-label">
              <p><strong>UniPi</strong></p>
            </div>
            <div class="item-label">
              <p><strong>UniPi-Replan</strong></p>
            </div>
          </div>
        </div>
        <div class="item">
          <div class="item-container">
            <p>
              <strong>Open the middle drawer of the cabinet</strong>: UniPi-Replan's diffusion model generates subgoals corresponding to a different task, indicating the increased difficulty of closed-loop video generation.
            </p>
          </div>
          <div class="item-container">
            <video class="validation" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/unipi/goal-0-succ.mp4"
                      type="video/mp4">
            </video>
            <video class="validation" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/unipi-replan/goal-0-wrong-task.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item-container">
            <div class="item-label">
              <p><strong>UniPi</strong></p>
            </div>
            <div class="item-label">
              <p><strong>UniPi-Replan</strong></p>
            </div>
          </div>
        </div>
        <div class="item">
          <div class="item-container">
            <p>
              <strong>Open the top drawer of the cabinet</strong>: Both UniPi and UniPi-Replan experience motor control failure, as it is difficult to tell when to close the gripper from noisy image subgoals.
            </p>
          </div>
          <div class="item-container">
            <video class="validation" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/unipi/goal-1-motor.mp4"
                      type="video/mp4">
            </video>
            <video class="validation" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/unipi-replan/goal-1-motor.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item-container">
            <div class="item-label">
              <p><strong>UniPi</strong></p>
            </div>
            <div class="item-label">
              <p><strong>UniPi-Replan</strong></p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <br>
  </div>
</section>
<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="titile">BibTeX</h2>
    <pre><code>@misc{lin2023spawnnet,
      title={SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks},
      author={Xingyu Lin and John So and Sashwat Mahalingam and Fangchen Liu and Pieter Abbeel},
      year={2023},
      eprint={2307.03567},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
    }</code></pre>
  </div>
</section> -->

<footer class="custom_footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p><br>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
